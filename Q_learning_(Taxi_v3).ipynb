{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Q-learning_(Taxi-v3).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO0Emf7NHAWSVyFN0hGyeRb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PLEX-GR00T/Maze_solving_MDP/blob/main/Q_learning_(Taxi_v3).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiGo951C9Krn"
      },
      "source": [
        "# CMPE249_HW3_RL_SmartCab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFNKssE89OdL"
      },
      "source": [
        "In this problem, you are going to implement the basic Q-learning algorithm to teach a Smartcab to pick up the passenger at one location and drop them off in another. The goals include:\n",
        "1. Drop off the passenger to the right location. \n",
        "2. Find the minimum path.\n",
        "3. Avoid obstacles and follow traffice rules.\n",
        "\n",
        "Fortunately, OpenAI Gym (https://gym.openai.com/) has a simualtion environment already built for this problem. \n",
        "\n",
        "You need to install \"gym\" first if you have not done so already using\n",
        "\n",
        "!pip install cmake 'gym[atari]' scipy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEPb5HDW9Zcr"
      },
      "source": [
        "Load the game environment and render what it looks like. \n",
        "The filled square represents the taxi, which is yellow without a passenger and green with a passenger.\n",
        "The pipe (\"|\") represents a wall which the taxi cannot cross.\n",
        "R, G, Y, B are the possible pickup and destination locations. The blue letter represents the current passenger pick-up location, and the purple letter is the current destination."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NU8zt_LswSeh",
        "outputId": "46064068-4542-422a-84ce-80bbe1398305"
      },
      "source": [
        "import gym\n",
        "env = gym.make(\"Taxi-v3\").env\n",
        "env.render()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | :\u001b[43m \u001b[0m|\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_t99auBC9idm"
      },
      "source": [
        "Here's the restructured problem statement (from Gym docs):\n",
        "\n",
        "\"There are 4 locations (labeled by different letters), and the job is to pick up the passenger at one location and drop him off at another. We receive +20 points for a successful drop-off and lose 1 point for every time-step it takes. There is also a 10 point penalty for illegal pick-up and drop-off actions.\"\n",
        "\n",
        "The action space include six actions:\n",
        "  0 = south\n",
        "  1 = north\n",
        "  2 = east\n",
        "  3 = west\n",
        "  4 = pickup\n",
        "  5 = dropoff\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "277pxWnkw3oI",
        "outputId": "5f34701f-a44f-4a24-e8b7-55e42efe21b0"
      },
      "source": [
        "# reset the environment to a new, random state\n",
        "env.reset() \n",
        "env.render()\n",
        "print(\"Action Space {}\".format(env.action_space))\n",
        "print(\"State Space {}\".format(env.observation_space))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: |\u001b[43m \u001b[0m: :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "\n",
            "Action Space Discrete(6)\n",
            "State Space Discrete(500)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_VhmFdX-dt9"
      },
      "source": [
        "When the Taxi environment is created, there is an initial Reward table that's also created, called `P`. We can think of it like a matrix that has the number of states as rows and number of actions as columns, i.e. a states Ã— actions matrix.\n",
        "\n",
        "Since every state is in this matrix, we can see the default reward values assigned to one of the state 328:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJOBDWHgw_jp",
        "outputId": "fae2d52b-48d4-45e8-fd3e-6b9bc360b87d"
      },
      "source": [
        "env.P[328]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: [(1.0, 428, -1, False)],\n",
              " 1: [(1.0, 228, -1, False)],\n",
              " 2: [(1.0, 348, -1, False)],\n",
              " 3: [(1.0, 328, -1, False)],\n",
              " 4: [(1.0, 328, -10, False)],\n",
              " 5: [(1.0, 328, -10, False)]}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvHEluKB-k57"
      },
      "source": [
        "This dictionary has the structure {action: [(probability, nextstate, reward, done)]}.\n",
        "\n",
        "A few things to note:\n",
        "\n",
        "1. The 0-5 corresponds to the actions (south, north, east, west, pickup, dropoff) the taxi can perform at our current state in the illustration.\n",
        "2. In this env, \"probability\" is always 1.0.\n",
        "3. The \"nextstate\" is the state we would be in if we take the action at this index of the dict\n",
        "4. All the movement actions have a -1 reward and the pickup/dropoff actions have -10 reward in this particular state. If we are in a state where the taxi has a passenger and is on top of the right destination, we would see a reward of 20 at the dropoff action (5)\n",
        "5. \"done\" is used to tell us when we have successfully dropped off a passenger in the right location. Each successfull dropoff is the end of an episode\n",
        "\n",
        "Note that if our agent chose to explore action two (2) in this state it would be going East into a wall. The source code has made it impossible to actually move the taxi across a wall, so if the taxi chooses that action, it will just keep accruing -1 penalties, which affects the long-term reward."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NskXSpp-hqo"
      },
      "source": [
        "Now, let's use Q-learning to solve this problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDh_stXl-vKF"
      },
      "source": [
        "First, we will initialize the Q-table to a 500 * 6 matrix of zeros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xW8LTbjNxJRd"
      },
      "source": [
        "import numpy\n",
        "q_table = np.zeros([env.observation_space.n, env.action_space.n])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a87Fm4tA-5gj"
      },
      "source": [
        "TODO: implement the Q-learning algorithm to find the best strategy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Rex27P8xOA4",
        "outputId": "bd15f7bb-b6bd-4702-8004-10b8aaf1f343"
      },
      "source": [
        "%%time\n",
        "\"\"\"Training the agent\"\"\"\n",
        "\n",
        "import random\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Hyperparameters\n",
        "alpha = 0.1\n",
        "gamma = 0.6\n",
        "epsilon = 0.1\n",
        "\n",
        "# For plotting metrics\n",
        "all_epochs = []\n",
        "all_penalties = []\n",
        "\n",
        "for i in range(1, 100001):\n",
        "    state = env.reset()\n",
        "\n",
        "    epochs, penalties, reward, = 0, 0, 0\n",
        "    done = False\n",
        "    \n",
        "    #TODO: implement the Q-learning algorithm\n",
        "    while not done:\n",
        "      if random.uniform(0,1) < epsilon:\n",
        "        action = env.action_space.sample()\n",
        "      else:\n",
        "        action = numpy.argmax(q_table[state])\n",
        "\n",
        "      nxtState, reward, done, info = new = env.step(action)\n",
        "\n",
        "      oldValue =  q_table[state,action]\n",
        "      nxtMax = numpy.max(q_table[nxtState])\n",
        "\n",
        "\n",
        "      newValue = (1-alpha) * oldValue + alpha * (reward + gamma * nxtMax)\n",
        "      q_table[state, action] = newValue\n",
        "\n",
        "      if reward == -10:\n",
        "        penalties += 1\n",
        "      \n",
        "      state = nxtState\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        clear_output(wait=True)\n",
        "        print(f\"Episode: {i}\")\n",
        "\n",
        "print(\"Training finished.\\n\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 100000\n",
            "Training finished.\n",
            "\n",
            "CPU times: user 1min 7s, sys: 14.2 s, total: 1min 21s\n",
            "Wall time: 1min 9s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lYtikRj-9Bd"
      },
      "source": [
        "let's see what the Q-values are at state 328:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJ7AmBDK8gD6",
        "outputId": "51959999-4c27-4c43-b16e-2f806b95a31d"
      },
      "source": [
        "q_table[328]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ -2.4131659 ,  -2.27325184,  -2.40018244,  -2.3604151 ,\n",
              "       -10.79628063, -10.67814863])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htkWC0Q68leV",
        "outputId": "93f03083-ffc5-48e1-d9b8-88a989e1f068"
      },
      "source": [
        "\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n",
        "\n",
        "total_epochs, total_penalties = 0, 0\n",
        "episodes = 100\n",
        "\n",
        "for _ in range(episodes):\n",
        "    state = env.reset()\n",
        "    epochs, penalties, reward = 0, 0, 0\n",
        "    \n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "        action = np.argmax(q_table[state])\n",
        "        state, reward, done, info = env.step(action)\n",
        "\n",
        "        if reward == -10:\n",
        "            penalties += 1\n",
        "\n",
        "        epochs += 1\n",
        "\n",
        "    total_penalties += penalties\n",
        "    total_epochs += epochs\n",
        "\n",
        "print(f\"Results after {episodes} episodes:\")\n",
        "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
        "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results after 100 episodes:\n",
            "Average timesteps per episode: 12.93\n",
            "Average penalties per episode: 0.0\n"
          ]
        }
      ]
    }
  ]
}