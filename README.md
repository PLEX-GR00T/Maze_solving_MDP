# Maze solving with different algorithms and their comparisons.
1) With Value Iteraiton and Policy Iteration
2) Q-learning 
3) Double-Q-learning
4) SARSA (State-Action-Reward-State-Action)

## 1) VI and PI

![image](https://github.com/PLEX-GR00T/Maze_solving_MDP/blob/main/Output%20Must%20Watch/iterations.gif)

## 2) Q-learning

<p float="left">
  <img src="https://github.com/PLEX-GR00T/Maze_solving_MDP/blob/main/Output%20Must%20Watch/Q-linear.png" width="500" />
  <img src="https://github.com/PLEX-GR00T/Maze_solving_MDP/blob/main/Output%20Must%20Watch/q-exponential.png" width="495" /> 
</p>

## 3) Double-Q-learning

<p float="left">
  <img src="https://github.com/PLEX-GR00T/Maze_solving_MDP/blob/main/Output%20Must%20Watch/Double%20q-linear.png" width="496" />
  <img src="https://github.com/PLEX-GR00T/Maze_solving_MDP/blob/main/Output%20Must%20Watch/Double%20q-exponential.png" width="500" /> 
</p>

## 4) SARSA

<p float="left">
  <img src="https://github.com/PLEX-GR00T/Maze_solving_MDP/blob/main/Output%20Must%20Watch/SARSA%20Linear.png" width="500" />
  <img src="https://github.com/PLEX-GR00T/Maze_solving_MDP/blob/main/Output%20Must%20Watch/SARSA%20Exponential.png" width="493" /> 
</p>

# SARSA using Q-learning

<p float="left">
  <img src="https://github.com/PLEX-GR00T/Maze_solving_MDP/blob/main/Output%20Must%20Watch/input%20maze.png" width="200" height="200" />
  <img src="https://github.com/PLEX-GR00T/Maze_solving_MDP/blob/main/Output%20Must%20Watch/output%20maze.png" width="200" height="200"/> 
  <img src="https://github.com/PLEX-GR00T/Maze_solving_MDP/blob/main/Output%20Must%20Watch/Q-learning%20and%20SARASA.png" width="400" height="200" />
</p>

We will collect the rewards for 5 runs and plot them together to see any underlying patterns.

![image](https://github.com/PLEX-GR00T/Maze_solving_MDP/blob/main/Output%20Must%20Watch/Q-learning%20and%20SARASA.png)
